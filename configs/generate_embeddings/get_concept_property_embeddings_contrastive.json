{
    "experiment_name": "get_embeds",
    "log_dirctory": null,
    "dataset_params": {
        "dataset_name": "dummy_concepts_contra",
        "hf_tokenizer_name": "bert-base-uncased",
        "hf_tokenizer_path": null,
        "concept_max_len": 32,
        "property_max_len": 32,
        "add_context": true,
        "context_num": 6,
        "loader_params": {
            "batch_size": 128,
            "num_workers": 4,
            "pin_memory": true
        }
    },
    "model_params": {
        "model_name": null,
        "hf_checkpoint_name": "bert-base-uncased",
        "hf_model_path": null,
        "vector_strategy": "mask_token"
    },
    "inference_params": {
        "pretrained_model_path": "/scratch/c.scmag3/contrastive_biencoder/trained_models/bienc_cnetp_pretrained/contastive_bienc_cnetp_pretrain_bert_base_uncased_ep4_bs32_wr0.15_wd0.9_tau0.1_lr1e-05_do0.4.pt",
        "get_con_prop_embeds": true,
        "input_data_type": "concept",
        "input_file_name": "data/generate_embeddding_data/dummy_concepts.txt",
        "save_dir": "trained_models/embeddings"
    }
}